|backbone|pretrain dataset| paper | download ||
|:---:|---|---|:---:|---|
|CaiT|ImageNet 1k| [Going deeper with Image Transformers](https://arxiv.org/pdf/2103.17239.pdf) |[weights](https://github.com/facebookresearch/deit)||
|CoaT|ImageNet 1k|[Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/pdf/2104.06399.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/coat.py)||
|ConViT|ImageNet 1k|[ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/pdf/2103.10697.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/convit.py#L51)||
|CSPNet|ImageNet 1k|[CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN](https://arxiv.org/pdf/1911.11929.pdf)|[cspresnet50](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/cspnet.py#L40)||
|DenseNet|ImageNet 1k||[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/densenet.py#L32)||
|dla60_res2net|ImageNet 1k|[DLA](https://arxiv.org/pdf/1707.06484.pdf) [Res2Net](https://arxiv.org/pdf/1904.01169.pdf) |[dla60_res2net](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dla.py#L44)||
|DPN|ImageNet 1k|[Dual Path Networks](https://arxiv.org/pdf/1707.01629.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dpn.py#L35)||
|EfficientNet|ImageNet 1k/21k|[EfficientNet-V2](https://arxiv.org/abs/2104.00298)|[tf_efficientnetv2_s_in21k](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/efficientnet.py#L350)||
|GhostNet|ImageNet 1k|[GhostNet](https://arxiv.org/pdf/1911.11907.pdf)|[ghostnet_100](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/ghostnet.py#L38)|CVPR 2020|
|gluon_resnet|ImageNet 1k||[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/gluon_resnet.py#L25)||
|HRNet|ImageNet 1k|[Deep High-Resolution Representation Learning for Visual Recognition](https://arxiv.org/pdf/1908.07919.pdf)|[weights](https://github.com/HRNet/HRNet-Image-Classification)|TPAMI|
|MLP-Mixer|ImageNet 1k/21k|[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/mlp_mixer.py#L64)||
|MobileNet V3|ImageNet 1k/21k|[Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/mobilenetv3.py#L39)|ICCV 2019|
|NFNet, NF-RegNet, NF-ResNet|ImageNet 1k|[Characterizing signal propagation to close the performance gap in unnormalized ResNets](https://arxiv.org/pdf/2101.08692.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/nfnet.py#L46)|ICLR 2021|
|PiT|ImageNet 1k|[Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/pdf/2103.16302.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/pit.py#L41)||
|RegNet|ImageNet 1k|[Designing Network Design Spaces](https://arxiv.org/pdf/2003.13678.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/regnet.py#L70)|CVPR 2020|
|Res2Net,Res2NeXt|ImageNet 1k|[Res2Net](https://arxiv.org/pdf/1904.01169.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/res2net.py#L29)|IEEE TPAMI 2021|
|ResNeSt|ImageNet 1k|[ResNeSt: Split-Attention Networks](https://arxiv.org/pdf/2004.08955v2.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnest.py#L29)||
|ResNet|||||
|ResNet v2|ImageNet 1k/21k||[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnetv2.py#L70)||
|ReXNet||[Rethinking Channel Dimensions for Efficient Model Design](https://arxiv.org/pdf/2007.00992.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/rexnet.py#L32)| CVPR 2021|
|Selective Kernel Networks (ResNet base)|ImageNet 1k|[Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network](https://arxiv.org/pdf/2001.06268.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/sknet.py#L33)||
|Swin Transformer|ImageNet 1k/21k|[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/swin_transformer.py#L43)||
|Transformer in Transformer|ImageNet 1k|[Transformer in Transformer](https://arxiv.org/pdf/2103.00112.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/tnt.py#L35)||
|TResNet|ImageNet 1k/21k|[TResNet: High Performance GPU-Dedicated Architecture](https://arxiv.org/pdf/2003.13630.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/tresnet.py#L30)||
|Twins|ImageNet 1k|[Twins: Revisiting the Design of Spatial Attention in Vision Transformers](https://arxiv.org/pdf/2104.13840.pdf)|[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/twins.py#L41)||
|ViT|ImageNet 1k/21k||||
|Vit-Hybrid|ImageNet 1k/21k||[weights](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer_hybrid.py#L37)||